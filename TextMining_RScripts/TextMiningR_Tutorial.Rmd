---
title: "A brief introduction to text mining in R"
author: "Shelly Lachish"
date: "1/04/2021"
output: 
  html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message = FALSE, warning = FALSE)
library(here)
library(tidyverse)
library(lubridate)
library(readtext)
library(quanteda)
```

## STEP 1: Importing text data: package(readtext)

The **readtext** function from the package with the same name, detects the file formats of a given list of files and extracts the content into a data.frame. **readtext** supports .txt, .json, .csv, .tab, .tsv, .xml, .pdf, .doc, .docx, .odt, .rtf, files from URLs and archive file (.zip, .tar, .tar.gz, .tar.bz). It can read in multiple files at once and supports glob/wildcard expressions, and allows you to set metadata variables by splitting on file name or path names (with the parameter *docvarsfrom*).

```{r importing_text_data}
#Import text data files
list.files(here('Working_Data/import_data_types/'))

#read in data
extracted_texts<- readtext(here('Working_Data/import_data_types/*'))
extracted_texts
extracted_texts$text[1]
cat(substr(extracted_texts$text[1] , 0, 1000))

#Set metadata with docvarsfrom (from file names - NOTE File names need to be standardised across list - same num/type separator)
extracted_texts <- readtext(here('Working_Data/import_data_types/Mindy*'), 
                            docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "place", "year"))
extracted_texts

#Get pdfs
extracted_texts <- readtext(here('Working_Data/import_data_types/*.pdf'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "journal", "year"))
extracted_texts
str(extracted_texts)
summary(extracted_texts)
```

-   A note on [**file encodings**]{.ul}: "*As encoding can also be a challenging issue for those reading in texts, we include functions for diagnosing encodings on a file-by-file basis, and allow you to specify vectorized input encodings to read in file types with individually set (and different) encodings. (All encoding functions are handled by the stringi package.)*"

-   If you are working specifically with scientific articles (pdfs) then you might also be interested in the **fulltext, pdftools**, and **pdfsearch** Rpackages.

## Load our twitter data

Here we will use a data set of \~7,000 tweets with the subject "covid19" (obtained using the rtweet package from the 8th - 15th Feb) to:

-   understand concepts in text analysis (corpus, tokenisation, document feature matrix)

-   do some basic text exploration (word frequencies, wordclouds, keywords in context)

-   perform a sentiment analysis on the content of the tweets

```{r load_tweet_data, message = FALSE}
#Load the data
covid_data<-read_tsv(here('Working_Data/covid19_tweets.tsv'))
summary(covid_data)
covid_data$text[1:4]
```

##### *ðŸ“– go to slides - Regular Expressions*

```{r clean_tweet_data, message = FALSE}
#Clean the dataset - string manipulation to preprocess the data
covid_data_clean <- covid_data %>% 
  #Replace @mentions
  mutate(text_clean = str_replace_all(text, '@\\S+', ' ')) %>%
  #Replace URLs
  mutate(text_clean = str_replace_all(text_clean, 'http\\S+', ' ')) %>%  
  #Replace new line indicators and re-tweet indicators
  mutate(text_clean = str_replace_all(text_clean, '\\n|\\r', ' ')) %>%
  #Replace ampersand indicators
  mutate(text_clean = str_replace_all(text_clean, '&amp', 'and')) %>%
  #Replace unicode character indicators <U+0001F449> (e.g. used for emojis)
  #NOTE: Take Care here with non-english text as some non-english characters may be represented as unicode characters!
  mutate(text_clean = str_replace_all(text_clean, '<U.*>', ' ')) %>%
  #trim leading/tailing white space characters
  mutate(text_clean = trimws(text_clean, which = 'both')) %>%     
  #Add day of week variable
  mutate(week_day = lubridate::wday(ymd_hms(created_at), label = T)) %>%
  #Add UK vs US location variable
  mutate(location_US_UK = ifelse(str_detect(location, regex("uk|britain|england|scotland|wales", ignore_case = T)), "UK", 
                                 ifelse(str_detect(location, regex("us|usa|america*", ignore_case = T)), "US", NA))) %>%
  select (-text, -created_at)

covid_data_clean$text_clean[1:4]
```

##### *ðŸ“– go to slides - Build the Corpus*

## STEP 2: From data to corpus

```{r build_corpus}
#Create your corpus
covid_corpus<-corpus(covid_data_clean, text_field = 'text_clean')
covid_corpus
head(docvars(covid_corpus))

#Inspecting the corpus
texts(covid_corpus)[2]

#Can get and save a summary of the first n records
(corp_summary <- summary(covid_corpus, n = 50))

#plot from this (but only summary of first 'n' texts)
ggplot(data = corp_summary, aes(x = log(followers_count), y = Tokens, group = 1)) + geom_line() + geom_point()  + theme_bw()
```

The corpus can be manipulated in a variety of ways (analogous to a data frame).

-   subset, concatenate, trim components from the text body

-   change the unit of texts between documents, paragraphs and sentences

-   extract segments of texts and tags from documents (useful when you analyze sections of documents or transcripts separately)

-   perform a keywords-in-context search (kwic function).

```{r corpus_manipulations}
#Corpus manipulations
#Subset a corpus
corp_monday <- corpus_subset(covid_corpus, week_day == 'Mon')
ndoc(corp_monday)

#concatenate two corpus
corp_tuesday <- corpus_subset(covid_corpus, week_day == 'Tue')
ndoc(corp_tuesday)
new_corp <- corpus(corp_tuesday + corp_monday)
ndoc(new_corp)

#change unit length to sentences/paragraphs/ or document (default)
corp_sentences <- corpus_reshape(corp_monday, to = "sentences")
ndoc(corp_sentences)
summary(corp_sentences, n = 7)

#The kwic function (keywords-in-context) performs a search for a word and allows us to view the contexts in which it occurs:
head(kwic(covid_corpus, pattern = 'vaccin*'))
head(kwic(covid_corpus, pattern = phrase("conspiracy theor*")))
texts(covid_corpus)[1649]
texts(covid_corpus)[2365]
```

##### *ðŸ“– go to slides*

## STEP 3: Tokenisation

The function `tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries. Usually a corpus is passed to `tokens()`, but it works with a character string too. By default, `tokens()` only removes separators (typically white spaces), but you can set other separators. You can remove punctuation and numbers prior to tokenisation.

You can remove tokens that you are not interested in using `tokens_select()`. Usually we remove grammatical words that have little or no substantive meaning in pre-processing. These are known as **stopwords**. The function `stopwords()` returns a pre-defined list of function words.

```{r get_tokens}
#Get Tokens

#Describe a bit how tokens are interpreted
(txt <- c(text1 = 'This is $10 in 999 different ways,\n up and down; left and #right!', 
         text2 = '@shellstar working: @ something on #Rtexttutorial 2day\t4ever !'))
(tokens<-tokens(txt))
(tokens<-tokens(txt, remove_punct = TRUE, remove_symbols = T, remove_numbers = T))
ntoken(tokens)

#----------------------------------------
#Get tokens from covid_tweets data
#----------------------------------------
#Remove punctuation, symbols, numbers
head(covid_toks<-tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T))

#Remove stopwords
head(covid_toks_nostop <- tokens_remove(covid_toks, pattern = stopwords("english")))

#Select or remove particular interesting words
head(covid_toks_vaccination <- tokens_select(covid_toks, pattern = c("vaccin*", "immunis*")))
head(covid_toks_vaccination <- tokens_select(covid_toks, pattern = c("vaccin*", "immunis*"), selection = 'remove'))

#-------- ngrams (multiword expressions)
tokens_ngrams(covid_toks, n = 2) #options to change concatenator, and skip distance
```

It is possible to create multi-word tokens (ngrams). One way is via the function `tokens_ngrams(covid_toks, n = 2)`. Another is using the function `tokens_compound()`. For example, `tokens_compound(covid_toks, pattern = phrase(c('Boris Johnson','hospital admission*')))`.

You can discover multiword expressions in your tokens using `textstat_collocations()`. This kind of thing is also useful if you want to create ngrams - particularly useful for looking for negative bigrams: `toks_neg_bigram<-tokens_compound(toks, pattern = phrase("not *"))` and then once you've found them you can select them from your corpus: `toks_neg_bigram_select <- tokens_select(toks_neg_bigram, pattern = phrase("not_*"))`

##### *ðŸ“– go to slides*

## STEP 4: Build the dfm (document feature matrix)

The function `dfm()` constructs a document-feature matrix (DFM) [from a tokens object or a corpus object]{.ul} (in which case the tokenisation occurs internally). `topfeatures()` will list the features (terms/tokens) in descending order. `textstat_frequency()` shows both term and document frequencies. You can also use the function to find the most frequent features within groups.

```{r get_dfm}
#So let's get our tokens again -- 
covid_toks <- tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T)
#- we now build the dfm
dfm(covid_toks)

#Remove stopwords and bespoke list of non-informative words
my_stopwords<-c('covid*', 'corona*', '#covid*', '#corona*', '#sars*')

(dfmat_covidtweets <- dfm(covid_toks, tolower = TRUE, remove = c(stopwords("english"), my_stopwords)))

#Can build this straight from the corpus without first generating tokens
#dfmat_covidtweets <- dfm(covid_corpus, tolower = FALSE, remove = c(stopwords("english"), my_stopwords))

docvars(dfmat_covidtweets)
  ndoc(dfmat_covidtweets)
  nfeat(dfmat_covidtweets)
  head(docnames(dfmat_covidtweets))
  head(featnames(dfmat_covidtweets))
  head(rowSums(dfmat_covidtweets), 10)
  head(colSums(dfmat_covidtweets), 10)
  sparsity(dfmat_covidtweets)

topfeatures(dfmat_covidtweets, 1000)

#For analysis purposes it is often a very good idea to remove very sparse terms from the dfm
(dfmat_covidtweets<-dfmat_covidtweets %>% 
                    dfm_trim(min_termfreq = 10, verbose = FALSE))
```

As with `tokens()`, you can select features from a DFM using `dfm_select()`. You can also select features based on the length of features (e.g. keep features consisting of at least five characters: `dfm_keep(dfmat_inaug, min_nchar = 5)`). While `dfm_select()` selects features based on patterns, `dfm_trim()` does this based on feature frequencies. If `min_termfreq = 10`, features that occur less than 10 times in the corpus are removed. If you want to convert the frequency count to a proportion within documents, use `dfm_weight(scheme = "prop")`

## Step 5: Exploration/analysis

```{r text_exploration}

tstat_freq <- textstat_frequency(dfmat_covidtweets)
head(tstat_freq, 5)
tail(tstat_freq, 5)

dfmat_covidtweets %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

#Let's take a look at a word cloud
set.seed(132)
textplot_wordcloud(dfmat_covidtweets, max_words = 100)
```

##### SHOULD WE STEM TERMS?

We have 'vaccine', 'vaccines', 'vaccinated', and 'vaccination' as top terms - but these are all essentially the same word.

ðŸ“– go to slides

```{r text_exploration_2}
(dfmat_covidtweets_stem <- dfm(covid_toks, tolower = TRUE, remove = c(stopwords(), my_stopwords), stem = TRUE) %>%
                              dfm_trim(min_termfreq = 10, verbose = FALSE))

dfmat_covidtweets_stem %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

textplot_wordcloud(dfmat_covidtweets_stem, max_words = 100)
```

## Differences between groups

```{r test_grouping_analysis}
#----------------------
#Look at grouping variables 
#----------------------
dfmat_covidtweets_gpd<-dfm_group(dfmat_covidtweets_stem, groups = 'location_US_UK')

##It is possible to change the "weighting" of values in the dfm -- e.g.
##dfmat_covidtweets_gpd<-dfm_weight(dfmat_covidtweets_gpd, scheme = 'prop')

#Plot group differences
gpd_diffs<- dfmat_covidtweets_gpd %>% textstat_frequency(n = 30, groups = 'location_US_UK')

ggplot(data = gpd_diffs, aes(x = factor(nrow(gpd_diffs):1), y = frequency)) +
    geom_point() +
    facet_wrap(~ group, scales = "free") +
    coord_flip() +
    scale_x_discrete(breaks = nrow(gpd_diffs):1,labels = gpd_diffs$feature) +
    labs(x = NULL, y = "Relative frequency")

#----------------------
#Subset the doc feature matrix
#----------------------
#Let's subset the dfm to look at popular hastags
dfmat_hashtags <- dfm_select(dfmat_covidtweets_gpd, pattern = '#*') 
textplot_wordcloud(dfmat_hashtags, max_words = 100)
textplot_wordcloud(dfmat_hashtags, comparison = TRUE, max_words = 100, color = c("blue", "red"))
```

## Sentiment analysis of the twitter data

##### *ðŸ“– go to slides*

The aim of sentiment analysis is to determine the polarity of a text (i.e., whether the emotions expressed in it are rather positive or negative). This is often done by word lists and by counting terms that were previously assigned to the categories *positive* or *negative.* Sometimes a third category is included; *neutral,* and sometimes words can also be assigned a sentiment strength, or be assigned to emotions like joy, anger, sadness, and so forth.

So basically a dictionary ('sentiment/topic dictionary') is used to group a number of individual terms into a 'sentiment' category, then the content of the whole text is calculated as the sum of the sentiment content of the individual words. (This isn't the only way to approach sentiment analysis, but it is an often-used approach). The quanteda package provides access to four general-purpose sentiment dictionaries (for English). All of these lexicons are based on unigrams (i.e., single words), and some bigrams.

```{r sentiment_analysis, message=F}
library(quanteda.sentiment)

#Dictionary based sentiment analysis
lengths(data_dictionary_LSD2015)
print(data_dictionary_LSD2015, max_nval = 10)

#Can also make and use your own bespoke dictionary
#my_dictionary <- dictionary(list(pos = c("happiness", "joy", "light"), neg = c("sadness", "anger", "darkness")))

#Set the polarity categories we wish to use
my_polarity <- list(pos = c("positive", "neg_negative"), neg = c("negative", "neg_positive"))
polarity(data_dictionary_LSD2015) <- my_polarity

#Get sentiment scores on our original corpus
sentiment<-textstat_polarity(dfmat_covidtweets, dictionary = data_dictionary_LSD2015)
summary(sentiment)
head(sentiment)

#There are several functions available for calculating polarity, and you can even supply your own with the option 'fun = xxx'
#If you have very long documents - you may want to use a function that weights sentiment values by doc length (total words) 

#--------------
#Let's see what is happening in the assignment
covid_toks[1:2]
tokens_lookup(covid_toks, data_dictionary_LSD2015, nested_scope = "dictionary", exclusive = FALSE)[1:2]
#--------------

#Create data for plotting - Sentiment by week-day and location
sentiment_dat<- cbind(sentiment, docvars(covid_corpus)) %>%
  filter(!is.na(location_US_UK)) %>% 
  mutate(week_day = fct_relevel(week_day, 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')) %>% 
  group_by(location_US_UK, week_day) %>% 
  summarise(mean_sent = mean(sentiment))
 
ggplot(data = sentiment_dat, aes(y = mean_sent, x = week_day, group = location_US_UK, color = location_US_UK))+
  geom_line()+
  geom_hline(yintercept = 0)+
  theme_bw()
```

## Other Dictionaries

```{r bing_dictionary}
#Sentiment "type" dictionary
lengths(data_dictionary_NRC)
print(data_dictionary_NRC, max_nval = 10)

#Alternative Method - use function tokens_lookup to assign sentiment to tokens
covidtweets_new_lsd <- tokens_lookup(covid_toks, dictionary = data_dictionary_NRC )
dfmat_sentiment_gpd <- dfm(covidtweets_new_lsd) %>% dfm_group(c('location_US_UK'))
dfmat_sentiment_gpd

#But I know my dataset has many more tweets from the UK than from the US - so need to weight the counts!
dfm_sentiment_prop <- dfm_weight(dfmat_sentiment_gpd, scheme = "prop")

plot_dat <- dfm_sentiment_prop %>% 
  convert(., to ="data.frame") %>% 
  pivot_longer(!doc_id, names_to = "sentiment_types", values_to = "proportions")

ggplot(data = plot_dat, aes(x=sentiment_types, y=proportions, fill=doc_id)) + 
  geom_bar(stat="identity", width=0.7, position=position_dodge(width=0.8)) +
  theme_bw()+
  theme(legend.title = element_blank())+
  theme(axis.title.x = element_blank())
```

## Case Study: Using supervised machine learning to build a text classifier

##### *ðŸ“– go to slides*

```{r load_extra_packages}
#load the text mining, ML classification packages
#quanteda.textmodels package includes models and classifiers for sparse matrix objects representing textual data (includes methods for correspondence analysis, latent semantic analysis, Naive Bayes and linear 'SVMs'  designed for sparse textual data)
library(quanteda.textmodels)
library(caret)
```

```{r prep_for_ML, message=F}
#*************************************************************
# Read in data for building the ML model ====
coded_freetext<-read_tsv(here('Working_Data/Supervised_ML/labelled_dataset.txt'))
head(coded_freetext)
dim(coded_freetext)

(labels_meanings<- coded_freetext %>%  
            select(label, meaning) %>% 
            mutate(label = as.character(label)) %>%  
            distinct() %>% 
            arrange(label))

#*************************************************************
# Create the corpus ====
freetext_corpus<-corpus(coded_freetext, text_field = 'freetext_clean')
#see bits of the corpus:
#texts(freetext_corpus)[9:11]

#*************************************************************
# Build the Document Feature Matrix ====
dfm <- dfm(freetext_corpus, verbose = T, remove_punct = T, remove_numbers = T, remove_symbols = T, stem = T, remove = stopwords('english')) 

#remove low freq words and terms only found in a few documents
(dfm_nolowfreqs <-dfm_trim(dfm, min_termfreq = 5, min_docfreq = 3))
```

```{r build_SVMmodels, message=F}
#*************************************************************
#Split the data into training and testing datasets 
# -- 60% will become our training data set / 40% for our testing data set 
#set the seed so random draws are reproducible
set.seed(4321)
training_data <- dfm_sample(dfm_nolowfreqs, size = 0.6*nrow(dfm_nolowfreqs))
testing_data<-dfm[setdiff(docnames(dfm_nolowfreqs), docnames(training_data)), ]

#run SVM classifier model on training data ====
svm_model <- textmodel_svm(training_data, training_data$label)

#Get the predictions (force = T means to only use features that exist in the both training and prediction dfms)
svm_predicted_class <- predict(svm_model, newdata = testing_data, type = "class", force = T)
head(svm_predicted_class)

#Extract the actual classifications
svm_actual_class <- testing_data$label

#Build the confusion matrix
(svm_tab_class <- table(svm_predicted_class, svm_actual_class))
sum(diag(svm_tab_class))/dim(testing_data)[1]

#Look at accuracy in more detail (from caret package)
confusionMatrix(svm_tab_class, mode = "everything")
```

```{r apply_SVMclassifier, message=F}
#***********************************************************************
# APPLY SVM model to predict classes of new freetext data fields ====

#load the new data that needs labelling
newfields_data<-read_tsv(here('Working_Data/Supervised_ML/unlabelled_dataset.txt'))
head(newfields_data)

#create the corpus
newfields_corpus<-corpus(newfields_data, text_field = 'freetext_clean') 
newdata_dfm <- dfm(newfields_corpus, verbose = T, remove_punct = T, remove_numbers = T,   
                   remove_symbols = T, stem = T, remove = c(stopwords('english'))) %>%  
               dfm_trim(min_termfreq = 5, min_docfreq = 3)

#Predict new classes for the new fields
newfields_predicted_label <- predict(svm_model, newdata = newdata_dfm, type = "class")
newfields_predicted_label[1:10]

#bind the classifications to the original data
model_classified_newfields<-cbind(newfields_data, newfields_predicted_label) %>% 
          mutate(label = as.character(newfields_predicted_label)) %>% 
          left_join(labels_meanings, by = 'label') %>% 
          select(-doc_id, -newfields_predicted_label)
model_classified_newfields
```

##### *ðŸ“– go to slides*
